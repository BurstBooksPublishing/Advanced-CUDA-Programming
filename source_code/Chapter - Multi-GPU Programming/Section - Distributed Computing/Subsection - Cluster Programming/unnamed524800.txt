// CUDA code for distributed multi-GPU programming
#include 
#include 
#include 

__global__ void kernel(float* data, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        data[idx] = data[idx] * 2.0f; // Example computation
    }
}

int main(int argc, char** argv) {
    MPI_Init(&argc, &argv); // Initialize MPI

    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank); // Get current process rank
    MPI_Comm_size(MPI_COMM_WORLD, &size); // Get total number of processes

    int N = 1024; // Data size per GPU
    float* h_data = new float[N]; // Host data
    float* d_data; // Device data

    cudaMalloc(&d_data, N * sizeof(float)); // Allocate device memory

    // Initialize host data
    for (int i = 0; i < N; i++) {
        h_data[i] = static_cast(i + rank * N); // Unique data per rank
    }

    cudaMemcpy(d_data, h_data, N * sizeof(float), cudaMemcpyHostToDevice);

    // Launch kernel
    kernel<<<(N + 255) / 256, 256>>>(d_data, N);

    cudaMemcpy(h_data, d_data, N * sizeof(float), cudaMemcpyDeviceToHost);

    // Synchronize MPI processes
    MPI_Barrier(MPI_COMM_WORLD);

    // Example: Gather results to rank 0
    if (rank == 0) {
        float* all_data = new float[N * size];
        MPI_Gather(h_data, N, MPI_FLOAT, all_data, N, MPI_FLOAT, 0, MPI_COMM_WORLD);

        // Process gathered data (example)
        for (int i = 0; i < N * size; i++) {
            std::cout << all_data[i] << " ";
        }
        delete[] all_data;
    } else {
        MPI_Gather(h_data, N, MPI_FLOAT, nullptr, N, MPI_FLOAT, 0, MPI_COMM_WORLD);
    }

    // Cleanup
    cudaFree(d_data);
    delete[] h_data;
    MPI_Finalize();

    return 0;
}