__global__ void gradientDescentKernel(float* params, const float* grad, 
                                      float learning_rate, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        // Update parameters using gradient descent
        params[idx] -= learning_rate * grad[idx];
    }
}

__global__ void momentumOptimizerKernel(float* params, const float* grad, 
                                        float* velocity, float learning_rate, 
                                        float momentum, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        // Update velocity with momentum
        velocity[idx] = momentum * velocity[idx] - learning_rate * grad[idx];
        // Update parameters using velocity
        params[idx] += velocity[idx];
    }
}

__global__ void adamOptimizerKernel(float* params, const float* grad, 
                                   float* m, float* v, float learning_rate, 
                                   float beta1, float beta2, float epsilon, 
                                   int t, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        // Update biased first moment estimate
        m[idx] = beta1 * m[idx] + (1 - beta1) * grad[idx];
        // Update biased second moment estimate
        v[idx] = beta2 * v[idx] + (1 - beta2) * grad[idx] * grad[idx];
        // Compute bias-corrected first moment estimate
        float m_hat = m[idx] / (1 - powf(beta1, t));
        // Compute bias-corrected second moment estimate
        float v_hat = v[idx] / (1 - powf(beta2, t));
        // Update parameters using Adam optimizer
        params[idx] -= learning_rate * m_hat / (sqrtf(v_hat) + epsilon);
    }
}