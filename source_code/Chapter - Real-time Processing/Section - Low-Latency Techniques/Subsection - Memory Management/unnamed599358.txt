// CUDA kernel for low-latency memory management
__global__ void lowLatencyKernel(float* input, float* output, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        // Shared memory for low-latency access
        __shared__ float sharedMem[256];
        sharedMem[threadIdx.x] = input[idx];
        __syncthreads();
        
        // Perform computation with shared memory
        output[idx] = sharedMem[threadIdx.x] * 2.0f;
    }
}

// Host function to manage memory and launch kernel
void launchLowLatencyKernel(float* h_input, float* h_output, int size) {
    float *d_input, *d_output;
    
    // Allocate device memory
    cudaMalloc((void**)&d_input, size * sizeof(float));
    cudaMalloc((void**)&d_output, size * sizeof(float));
    
    // Copy input data to device
    cudaMemcpy(d_input, h_input, size * sizeof(float), cudaMemcpyHostToDevice);
    
    // Launch kernel with optimal block size for low latency
    int blockSize = 256;
    int gridSize = (size + blockSize - 1) / blockSize;
    lowLatencyKernel<<>>(d_input, d_output, size);
    
    // Copy result back to host
    cudaMemcpy(h_output, d_output, size * sizeof(float), cudaMemcpyDeviceToHost);
    
    // Free device memory
    cudaFree(d_input);
    cudaFree(d_output);
}