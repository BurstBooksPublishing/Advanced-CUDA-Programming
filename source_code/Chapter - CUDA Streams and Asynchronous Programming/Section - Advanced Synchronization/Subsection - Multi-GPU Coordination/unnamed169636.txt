#include 
#include 
#include 

#define CHECK(call) \
{ \
    const cudaError_t error = call; \
    if (error != cudaSuccess) { \
        std::cerr << "Error: " << __FILE__ << ":" << __LINE__ << ", " \
                  << cudaGetErrorString(error) << std::endl; \
        exit(1); \
    } \
}

void multiGPUCoordination(int numGPUs) {
    std::vector streams(numGPUs);
    std::vector d_data(numGPUs);
    std::vector h_data(numGPUs);

    // Initialize streams and allocate memory on each GPU
    for (int i = 0; i < numGPUs; ++i) {
        CHECK(cudaSetDevice(i));
        CHECK(cudaStreamCreate(&streams[i]));
        CHECK(cudaMalloc((void**)&d_data[i], sizeof(float) * 1024));
        h_data[i] = (float*)malloc(sizeof(float) * 1024);
    }

    // Perform computation on each GPU asynchronously
    for (int i = 0; i < numGPUs; ++i) {
        CHECK(cudaSetDevice(i));
        kernel<<<1, 1024, 0, streams[i]>>>(d_data[i]);
    }

    // Synchronize streams across all GPUs
    for (int i = 0; i < numGPUs; ++i) {
        CHECK(cudaSetDevice(i));
        CHECK(cudaStreamSynchronize(streams[i]));
    }

    // Copy data back to host and free resources
    for (int i = 0; i < numGPUs; ++i) {
        CHECK(cudaSetDevice(i));
        CHECK(cudaMemcpyAsync(h_data[i], d_data[i], sizeof(float) * 1024,
                              cudaMemcpyDeviceToHost, streams[i]));
        CHECK(cudaFree(d_data[i]));
        free(h_data[i]);
        CHECK(cudaStreamDestroy(streams[i]));
    }
}

__global__ void kernel(float* data) {
    int idx = threadIdx.x + blockIdx.x * blockDim.x;
    if (idx < 1024) {
        data[idx] = idx * 1.0f; // Simple computation
    }
}

int main() {
    int numGPUs;
    CHECK(cudaGetDeviceCount(&numGPUs));
    multiGPUCoordination(numGPUs);
    return 0;
}