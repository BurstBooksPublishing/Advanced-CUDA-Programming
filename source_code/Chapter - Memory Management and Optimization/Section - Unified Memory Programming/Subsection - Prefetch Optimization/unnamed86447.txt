// CUDA kernel to demonstrate prefetch optimization
__global__ void prefetchOptimizedKernel(int *data, int N) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < N) {
        // Prefetch data to cache for faster access
        __prefetch(&data[idx]);
        // Perform computation on the prefetched data
        data[idx] = data[idx] * 2;
    }
}

int main() {
    int N = 1 << 20; // Size of the data array
    int *h_data, *d_data;
    
    // Allocate unified memory
    cudaMallocManaged(&h_data, N * sizeof(int));
    cudaMalloc(&d_data, N * sizeof(int));
    
    // Initialize data on the host
    for (int i = 0; i < N; i++) {
        h_data[i] = i;
    }
    
    // Copy data to device
    cudaMemcpy(d_data, h_data, N * sizeof(int), cudaMemcpyHostToDevice);
    
    // Launch kernel with prefetch optimization
    int blockSize = 256;
    int numBlocks = (N + blockSize - 1) / blockSize;
    prefetchOptimizedKernel<<>>(d_data, N);
    
    // Copy result back to host
    cudaMemcpy(h_data, d_data, N * sizeof(int), cudaMemcpyDeviceToHost);
    
    // Free memory
    cudaFree(d_data);
    cudaFree(h_data);
    
    return 0;
}