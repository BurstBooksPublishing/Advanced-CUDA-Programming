// CUDA C++ code demonstrating cache hierarchy in modern GPUs
#include 
#include 

__global__ void cacheHierarchyKernel(int* data, int* result, int N) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < N) {
        // Access data with potential cache hits/misses
        int value = data[idx];
        
        // Perform a simple operation to utilize cache
        result[idx] = value * 2;
    }
}

int main() {
    const int N = 1024;
    int h_data[N], h_result[N];
    int *d_data, *d_result;

    // Initialize host data
    for (int i = 0; i < N; ++i) {
        h_data[i] = i;
    }

    // Allocate device memory
    cudaMalloc((void**)&d_data, N * sizeof(int));
    cudaMalloc((void**)&d_result, N * sizeof(int));

    // Copy data to device
    cudaMemcpy(d_data, h_data, N * sizeof(int), cudaMemcpyHostToDevice);

    // Launch kernel with 1 block and N threads
    cacheHierarchyKernel<<<1, N>>>(d_data, d_result, N);

    // Copy result back to host
    cudaMemcpy(h_result, d_result, N * sizeof(int), cudaMemcpyDeviceToHost);

    // Free device memory
    cudaFree(d_data);
    cudaFree(d_result);

    // Print result (for demonstration purposes)
    for (int i = 0; i < N; ++i) {
        std::cout << h_result[i] << " ";
    }
    std::cout << std::endl;

    return 0;
}