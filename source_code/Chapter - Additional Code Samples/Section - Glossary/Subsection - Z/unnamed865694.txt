// CUDA C++ code showcasing kernel fusion for efficient inference pipelines

__global__ void fused_kernel(float* input, float* weights, float* output, int N) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < N) {
        // Perform element-wise multiplication and accumulation
        float sum = 0.0f;
        for (int i = 0; i < N; ++i) {
            sum += input[i] * weights[idx * N + i];
        }
        // Apply activation function (ReLU)
        output[idx] = fmaxf(0.0f, sum);
    }
}

void inference_pipeline(float* h_input, float* h_weights, float* h_output, int N) {
    float *d_input, *d_weights, *d_output;
    
    // Allocate device memory
    cudaMalloc(&d_input, N * sizeof(float));
    cudaMalloc(&d_weights, N * N * sizeof(float));
    cudaMalloc(&d_output, N * sizeof(float));
    
    // Copy data to device
    cudaMemcpy(d_input, h_input, N * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_weights, h_weights, N * N * sizeof(float), cudaMemcpyHostToDevice);
    
    // Launch fused kernel
    int blockSize = 256;
    int numBlocks = (N + blockSize - 1) / blockSize;
    fused_kernel<<>>(d_input, d_weights, d_output, N);
    
    // Copy result back to host
    cudaMemcpy(h_output, d_output, N * sizeof(float), cudaMemcpyDeviceToHost);
    
    // Free device memory
    cudaFree(d_input);
    cudaFree(d_weights);
    cudaFree(d_output);
}